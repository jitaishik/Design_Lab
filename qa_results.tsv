S.No.	Gold Standard Answer	Student Answer	Prob for Entailment	Prob for Neutral	Prob for Contradiction
0	deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network. deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.	profound learning may be a portion of machine learning with an calculation propelled by the structure and work of the brain, which is called an fake neural organize.	0.36020988	0.4563447	0.18344541
1	ai stands for artificial intelligence. it is a technique which enables machines to mimic human behavior.machine learning is a subset of ai which uses statistical methods to enable machines to improve with experiences.deep learning is a part of machine learning, which makes the computation of multi-layer neural networks feasible. it takes advantage of neural networks to simulate human-like decision making.	ai stands for manufactured insights. it may be a procedure which enables machines to imitate human behavior.machine learning may be a subset of ai which employments measurable strategies to empower machines to make strides with experiences.deep learning may be a portion of machine learning, which makes the computation of multi-layer neural systems doable.	0.844482	0.13716082	0.018357214
2	supervised learning is a system in which both input and desired output data are provided. input and output data are labeled to provide a learning basis for future data processing.unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. the common unsupervised learning method is cluster analysis. it is used for exploratory data analysis to find hidden patterns or grouping in data.	administered learning could be a framework in which both input and wanted yield information are provided. input and yield information are labeled to supply a learning premise for future information processing.unsupervised method does not require labeling data unequivocally, and the operations can be carried out without the same.	0.9748493	0.011099603	0.014051172
3	both shallow and deep networks are good enough and capable of approximating any function. but for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. deeper networks can create deep representations. at every layer, the network learns a new, more abstract representation of the input.	both shallow and deep networks are capable of approximating any function and are good enough.deeper networks, on the other hand, can be substantially more efficient in terms of computation for the same level of accuracy. 	0.7186065	0.26718056	0.0142129725
4	overfitting is the most common issue which occurs in deep learning. it usually occurs when a deep learning algorithm apprehends the sound of specific data. it also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.	it ordinarily happens when a profound learning calculation secures the sound of particular information.	0.8404742	0.07908797	0.080437936
5	backpropagation is a training algorithm which is used for multilayer neural networks. it transfers the error information from the end of the network to all the weights inside the network. it allows the efficient computation of the gradient.backpropagation can be divided into the following steps.it can forward propagation of training data through the network to generate output. it uses target value and output value to compute error derivative concerning output activations.it can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers.it uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights.it updates the weights.	backpropagation may be a preparing calculation which is utilized for multilayer neural systems. it exchanges the mistake data from the conclusion of the organize to all the weights interior the organize. it permits the proficient computation of the gradient.it employments target esteem and yield esteem to compute mistake subordinate concerning yield activations.it can backpropagate to compute the derivative of the blunder concerning yield actuations within the past layer and proceed for all covered up layers.	0.2953396	0.23644105	0.4682193
6	fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. the software is created with a high-quality feature known as the special portrayal. one can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals.	one can viably utilize it to create real-time cluster information, which is amazingly supportive for preparing all categories of signals.	0.9535235	0.04535457	0.0011219014
7	there are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula.	independent design speaks to autonomous or unspecified numerical bases which are free from any particular categorizer or equation.	0.5713386	0.10998941	0.31867194
8	deep learning has brought significant changes or revolution in the field of machine learning and data science. the concept of a complex neural network (cnn) is the main center of attention for data scientists. it is widely taken because of its advantages in performing next-level machine learning operations. the advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. it is one of the rare procedures which allow the movement of data in independent pathways. most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.	the concept of a complex neural arrange (cnn) is the most center of consideration for information researchers. it is broadly taken since of its points of interest in performing next-level machine learning operations. the focal points of deep learning moreover incorporate the method of clarifying and streamlining issues based on an calculation due to its most extreme adaptable and versatile nature.	0.32725912	0.4078901	0.2648507
9	deep learning frameworks or tools are,tensorflow, keras, chainer, pytorch, theano & ecosystem, caffe2, cntk, dynetgensim, dsstne, gluon, paddle, mxnet, bigdl	there are a lot of tools.	0.8273931	0.17149797	0.0011089314
10	in neural networking, weight initialization is one of the essential factors. a bad weight initialization prevents a network from learning. on the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. biases can be initialized to zero. the standard rule for setting the weights is to be close to zero without being too small.	it refers to initialising weights to the model parameters which we want to train. its very important for convergence.	0.0058021504	0.99239135	0.0018064814
11	data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. it assures better convergence during backpropagation. in general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation.	information normalization bubbles down to subtracting the cruel of each information point and partitioning by its standard deviation.	0.89157933	0.06396162	0.044459168
12	if the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.as a result, the network cannot learn at all because there is no source of asymmetry between neurons. that is the reason why we need to add randomness to the weight initialization process.	the model cannot learn at all since there's no source of asymmetry between neurons. that's the reason why we ought to include arbitrariness to the weight initialization prepare.	0.9899232	0.008090044	0.001986795
13	the activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. without the activation function, the neural network would be only able to learn function, which is a linear combination of its input data.activation function translates the inputs into outputs. the activation function is responsible for deciding whether a neuron should be activated or not. it makes the decision by calculating the weighted sum and further adding bias with it. the basic purpose of the activation function is to introduce non-linearity into the output of a neuron.	enactment work deciphers the inputs into yields. the actuation work is mindful for choosing whether a neuron ought to be enacted or not. it makes the choice by calculating the weighted entirety and advance including inclination with it. it brings linearity to the network.	0.009592266	0.023000654	0.9674071
14	the binary step function is an activation function, which is usually based on a threshold. if the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. this function does not allow multi-value outputs.	its basically a activation function which gets activated once the value reaches a certain threshold, else it remains inactive.	0.020129755	0.9009358	0.07893432
15	the sigmoid activation function is also called the logistic function. it is traditionally a trendy activation function for neural networks. the input data to the function is transformed into a value between 0.0 and 1.0. input values that are much larger than 1.0 are transformed to the value 1.0. similarly, values that are much smaller than 0.0 are transformed into 0.0. the shape of the function for all possible inputs is an s-shape from zero up through 0.5 to 1.0. it was the default activation used on neural networks, in the early 1990s.	the logistic function is another name for the sigmoid activation function.it has long been a popular activation function for neural networks.the function's input data is converted to a value between 0.0 and 1.0. 	0.0012642172	0.9601512	0.038584486
16	the hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. it provides output values between -1.0 and 1.0. later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. it was easier to train and often had better predictive performance.	its a popular activation function which beings non-linearity to the neural network.	0.0035574406	0.9856677	0.010774974
17	a node or unit which implements the activation function is referred to as a rectified linear activation unit or relu for short. generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.adoption of relu may easily be considered one of the few milestones in the deep learning revolution.	its a popular activation function which beings non-linearity to the neural network.	0.0016965113	0.22566828	0.77263504
18	the leaky relu (lrelu or lrel) manages the function to allow small negative values when the input is less than zero.	when the input is less than zero, the leaky relu (lrelu or lrel) adjusts the function to enable minor negative values. 	0.97868997	0.016346851	0.0049633845
19	the softmax function is used to calculate the probability distribution of the event over 'n' different events. one of the main advantages of using softmax is the output probabilities range. the range will be between 0 to 1, and the sum of all the probabilities will be equal to one. when the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability.	the probability distribution is calculated using the softmax function.the probability range will be 0 to 1, with the sum of all probabilities equal to one. 	0.9708319	0.012038088	0.017130038
20	swish is a new, self-gated activation function. researchers at google discovered the swish function. according to their paper, it performs better than relu with a similar level of computational efficiency.	swish function is an activation function.	0.9769946	0.018897345	0.0041080206
21	relu function is the most used activation function. it helps us to solve vanishing gradient problems.	the most used activation function is the relu activation function.	0.99560803	0.0037946778	0.0005972616
22	no, relu function has to be used in hidden layers.	yes, relu can be used as an activation function on the output layer.	0.00028158387	0.0021929354	0.9975254
23	softmax activation function has to be used in the output layer.	softmax activation function is used in the ouput layer of a neural network. it gives the probability of various classes.	0.0012353009	0.9899934	0.008771228
24	autoencoder is an artificial neural network. it can learn representation for a set of data without any supervision. the network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. as a result, they can learn efficient ways of representing the data. autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs.	autoencoder is an automatic encoder. it gives the the embedding vectors without even training. it directly give the output encodings by learning the input. it only consists of an encoder.	0.55719024	0.19150573	0.2513041
25	dropout is a cheap regulation technique used for reducing overfitting in neural networks. we randomly drop out a set of nodes at each training step. as a result, we create a different model for each training case, and all of these models share weights. it's a form of model averaging.	dropout drops certain proportion of neurons in the network randomly. this helps in avoiding overfitting.	0.9682081	0.030866478	0.0009253874
26	a boltzmann machine (also known as stochastic hopfield network with hidden units) is a type of recurrent neural network. in a boltzmann machine, nodes make binary decisions with some bias. boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. boltzmann machines can be used to optimize the solution to a problem.	a boltzmann machine is a stochastic spin-glass model with an external field, that is a sherrington–kirkpatrick model, that is a stochastic ising model. it is a statistical physics technique applied in the context of cognitive science. it is also classified as markov random field.	0.0031636308	0.065953344	0.93088305
27	the capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. model capacity can approximate any given function. when there is a higher model capacity, it means that the larger amount of information can be stored in the network.	its the number of paramters in the model. the more the parameters in the model, the better it can hold information and aproximate any mathematical function.	0.0035912623	0.9934733	0.0029354752
28	a cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. it may depend on variables such as weights and biases.it provides the performance of a neural network as a whole. in deep learning, our priority is to minimize the cost function. that's why we prefer to use the concept of gradient descent.	cost function is used to determine how well a model is functioing. it is similar to loss function.	0.06367037	0.62982154	0.30650815
29	an optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. it's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function	gradient descent is an algorithm used to reduce the loss incurred by a model. it finds the gradient of the loss curve with respect to a weight or bias and travels down it, thus the name descent, to update weights and biases.	0.5541818	0.4119516	0.033866588
30	stochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example.batch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration.mini-batch gradient descent is a variation of stochastic gradient descent. instead of a single training example, mini-batch of samples is used. mini-batch gradient descent is one of the most popular optimization algorithms.	stochastic uses single datapoint to calculate gradient. batch gradient uses the all the datapoints where-as mini batch divides the dataset into batches and then calculates gradients of each batch.	0.46010184	0.073482126	0.46641594
31	it is computationally efficient compared to stochastic gradient descent.it improves generalization by finding flat minima.it improves convergence by using mini-batches. we can approximate the gradient of the entire training set, which might help to avoid local minima.	mini batch gradient descent uses less memory than batch gradient descent and gives better results than stochastic gradient descent.	0.005118742	0.98961276	0.005268451
32	element-wise matrix multiplication is used to take two matrices of the same dimensions. it further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.	element to element multiplication of two or more matrices.	0.8970574	0.055884864	0.047057804
33	a convolutional neural network, often called cnn, is a feedforward neural network. it uses convolution in at least one of its layers. the convolutional layer contains a set of filter (kernels). this filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. as a result of training, the network automatically learns filters that can detect specific features.	convolutional neural network is used for image data and uses convolution operation.	0.9454783	0.052470453	0.0020513579
34	convolution, relu, pooling and then full collectedness.	there are a lot of tools.	0.13839124	0.85839325	0.003215468
35	this layer comprises of a set of independent filters. all these filters are initialized randomly. these filters then become our parameters which will be learned by the network subsequently.	convolution is a mathematical operation of two signals to produce a third signal which signifies how one is modified by the other.	0.0041334033	0.39074537	0.60512114
36	the relu layer is used with the convolutional layer.	it is a piecewise linear function used for neural network training	0.0014906414	0.14230014	0.85620916
37	it reduces the spatial size of the representation to lower the number of parameters and computation in the network. this layer operates on each feature map independently.	it is an operation where we select a subset of elements of a tensor to pass on to the next layer of a neural network.	0.0018896192	0.84823704	0.14987333
38	neurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular neural networks. their activations can be easily computed with a matrix multiplication followed by a bias offset.	every node is connected to each other	0.09224766	0.8830262	0.024725996
39	rnn stands for recurrent neural networks. these are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. rnn use backpropagation algorithm for training because of their internal memory. rnn can remember important things about the input they received, which enables them to be very precise in predicting what's coming next.	neural network good at training language data	0.6924451	0.30309007	0.004464857
40	recurrent neural network uses backpropagation algorithm for training, but it is applied on every timestamp. it is usually known as back-propagation through time (btt).there are two significant issues with back-propagation,vanishing and exploding gradient.	coding is difficult.	0.0011084417	0.97983235	0.019059105
41	lstm stands for long short-term memory. it is an artificial rnn (recurrent neural network) architecture, which is used in the field of deep learning. lstm has feedback connections which makes it a "general purpose computer." it can process not only a single data point but also entire sequences of data.they are a special kind of rnn which are capable of learning long-term dependencies.	a modification to baseline rnn that allows them to retain long term dependencies.	0.266539	0.69972056	0.033740498
42	an autoencoder contains three layers- the encoder is used to compress the input into a latent space representation. it encodes the input images as a compressed representation in a reduced dimension. the compressed images are the distorted version of the original image.the code layer is used to represent the compressed input which is fed to the decoder.the decoder layer decodes the encoded image back to its original dimension. the decoded image is a reduced reconstruction of the original image. it is automatically reconstructed from the latent space representation.	autoencoders have two layers - encoder and decoder	0.013568686	0.026150212	0.960281
43	deep autoencoder is the extension of the simple autoencoder. the first layer present in deepautoencoder is responsible for first-order functions in the raw input. the second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. deeper layers which are available in the deep autoencoder tend to learn even high-order features.	autoencoders with ten or more layers are called deep autoencoder	0.0035078803	0.91165596	0.08483598
44	the procedure of developing an assumption structure involves three specific actions.the first step contains algorithm development. this particular process is lengthy.the second step contains algorithm analyzing, which represents the in-process methodology.the third step is about implementing the general algorithm in the final procedure. the entire framework is interlinked and required for throughout the process.	first step is understanding the problem requirements and deciding on an architecture suitable for such a task. next step is deciding the model size based on input complexity and available computational resources. the final step is to decide the hyperparameters.	0.0010975108	0.42797303	0.57092947
45	a perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. it is an algorithm for supervised learning of binary classifiers. this algorithm is used to enable neurons to learn and processes elements in the training set one at a time.	perceptron is a neural network used for training classification problems with binary labels.	0.8574645	0.11084128	0.031694233
46	single layer and multi layer perceptrons.	shallow and deep perceptrons.	0.026932798	0.06523918	0.9078281
47	a multi-layer perceptron (mlp) is one of the most basic neural networks that we use for classification. for a binary classification problem, we know that the output can be either 0 or 1. this is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.simply put, it is just the difference in the threshold function! when we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a perceptron model:	perceptron uses a step function and logistic regression uses a logistic function	0.33331048	0.1383316	0.528358
48	essentially, you can have a different bias value at each layer or at each neuron as well. however, it is best if we have a bias matrix for all the neurons in the hidden layers as well.a point to note is that both these strategies would give you very different results.	yes	0.029237118	0.67875046	0.2920124
49	in order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.	any neural network without any activation functions can be simplified to a single layer network.	0.0011464361	0.09432548	0.9045282
50	in simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. while this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.in short, there is no learning happening by the network! what do you call the phenomenon of the model being unable to learn any patterns from the data? yes, underfitting.therefore, if all weights have the same initial value, this would lead to underfitting.	training will procees normally	0.013344088	0.9712371	0.015418763
51	some example of supervised learning and deep learning include,image classification,text classification and sequence taggingon the other hand, there are some unsupervised deep learning techniques as well,word embeddings (like skip-gram and continuous bag of words): understanding word embeddings: from word2vec to count vectors.autoencoders: learn how to enhance a blurred image using an autoencoder!	supervised tasks include classification, regression, generation etc. unsupervised tasks mostly include some variant of clustering	0.0056513953	0.9761778	0.018170767
52	this is a question best explained with a real-life example. consider that you want to go out today to play a cricket match with your friends. now, a number of factors can affect your decision-making, like,how many of your friends can make it to the game?,how much equipment can all of you bring?,what is the temperature outside?and so on. these factors can change your decision greatly or not too much. for example, if it is raining outside, then you cannot go out to play at all. or if you have only one bat, you can share it while playing as well. the magnitude by which these factors can affect the game is called the weight of that factor.factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.however, does this mean that we can play a cricket match with only one bat? no – we would need 1 ball and 6 wickets as well. this is where bias comes into the picture. bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed.	weights and biases help us track our experiments by providing live monitoring of various useful metrics.	0.16718005	0.3774829	0.45533705
53	the common data structures used in deep learning are lists, matrix,dataframes, tensors and computation graphs.	tensor	0.07693871	0.8249189	0.098142326
54	batch normalization is one of the techniques used for reducing the training time of our deep learning algorithm. just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well.	batch normalisation performs regularisation	0.61663276	0.36085254	0.022514755
55	sigmoid, tanh, relu and softmax.	sigmoid, tanh, softmax, relu, gelu, leaky-relu, 	0.007783619	0.72055405	0.27166235
56	the main component that differentiates recurrent neural networks (rnn) from the other models is the addition of a loop at each node. this loop brings the recurrence mechanism in rnns. in a basic artificial neural network (ann), each input is given the same weight and fed to the network at the same time. so, for a sentence like “i saw the movie and hated it”, it would be difficult to capture the information which associates “it” with the “movie”.	rnns can exploit dependencies across time steps so they are useful with sequence data	0.013387094	0.9751998	0.011412998
57	this loop essentially includes a time component into the network as well. this helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.this is why the backpropagation in rnn is called backpropagation through time, as in backpropagation at each time step.	in rnn the gradients have to backpropagate through multiple time steps.	0.046489943	0.9259928	0.027517222
58	as you can see, the lstm model can become quite complex. in order to still retain the functionality of retaining information across time and yet not make a too complex model, we need grus.basically, in grus, instead of having an additional forget gate, we combine the input and forget gates into a single update gate.it is this reduction in the number of gates that makes gru less complex and faster than lstm.	since it has fewer components, it involves less computation	0.0034933814	0.98256373	0.013942735
59	advancements in deep learning have made it possible to solve many tasks in natural language processing. networks/sequence models like rnns, lstms, etc. are specifically used for this purpose – so as to capture all possible information from a given sentence, or a paragraph. however, sequential processing comes with its caveats.it requires high processing power.it is difficult to execute in parallel because of its sequential naturethis gave rise to the transformer architecture. transformers use what is called the attention mechanism. this basically means mapping dependencies between all the parts of a sentence.	attention mechanism used in transformer is parallelisable	0.2817753	0.47186074	0.24636391
60	deep learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. it performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).	learning multi layer neural networks to perform classification or generation tasks.	0.54956293	0.3234162	0.12702088
61	neural networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.	neural networks are a mathematical imitation of nural networks present in the human brain.	0.010502732	0.9680222	0.021474956
62	as in neural networks, mlps have an input layer, a hidden layer, and an output layer. it has the same structure as a single layer perceptron with one or more hidden layers. a single layer perceptron can classify only linear separable classes with binary output (0,1), but mlp can classify nonlinear classes.except for the input layer, each node in the other layers uses a nonlinear activation function. this means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. mlp uses a supervised learning method called “backpropagation.” in backpropagation, the neural network calculates the error with the help of cost function. it propagates this error backward from where it came (adjusts the weights to train the model more accurately).	multi-layer preceptron is a neural nework consisting multiple layers, that is input layer, hidden layers, and an output layer.	0.02295916	0.35638463	0.6206562
63	the process of standardizing and reforming data is called “data normalization.” it’s a pre-processing step to eliminate data redundancy. often, data comes in, and you get the same information in different formats. in these cases, you should rescale values to fit into a particular range, achieving better convergence.	data normalisation is the process of standardising the data to remove any biases that might occur from higher range of values in a feature.	0.0026315488	0.9962182	0.001150023
64	a feedforward neural network signals travel in one direction from input to output. there are no feedback loops; the network considers only the current input. it cannot memorize previous inputs (e.g., cnn).a recurrent neural network’s signals travel in both directions, creating a looped network. it considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory.	rnns can work on data spanning across mutiple time steps while anns are unable to do so.	0.026923722	0.8030635	0.17001273
65	the rnn can be used for sentiment analysis, text mining, and image captioning. recurrent neural networks can also address time series problems such as predicting the prices of stocks in a month or quarter.	can be used for language, speech and time-series data	0.30145654	0.46428585	0.23425764
66	with neural networks, you’re usually working with hyperparameters once the data is formatted correctly. a hyperparameter is a parameter whose value is set before the learning process begins. it determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).	hyperparameters are decided by the users and decide how the traning process will be carried out. the various hyperparameters are learning rate, batch size, number of epochs, etc.	0.012665837	0.81648463	0.17084941
67	when your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. it will take many updates before reaching the minimum point.if the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. it may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).	if the learning rate is set too low them the model will take a very long time to converge. if it is set too high then it will start oscillating around the optimal point but never reach the optimal point.	0.0017270452	0.9842875	0.013985377
68	dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). it doubles the number of iterations needed to converge the network.batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.	they provide regularising effect	0.2395203	0.7423362	0.01814357
69	overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. it is more likely to occur with nonlinear models that have more flexibility when learning a target function. an example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. it might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. the model performs well on training data, but not in the real world.underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. this usually happens when there is less and incorrect data to train a model. underfitting has both poor performance and accuracy.to combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model.	overfitting is following the training data too closely as a result it struggles with out of distribution data that is the test dataset. in underfitting the model fit both the trining and test data badly. to avoid overfitting we can increase the amount of train data or use methods like dropout or regularisation. to avoid underfitting we can increase the model complexity.	0.341983	0.31389356	0.3441233
70	there are two methods here: we can either initialize the weights to zero or assign them randomly.initializing all weights to 0: this makes your model similar to a linear model. all the neurons and every layer perform the same operation, giving the same output and making the deep net useless.initializing all weights randomly: here, the weights are assigned randomly by initializing them very close to 0. it gives better accuracy to the model since every neuron performs different computations. this is the most commonly used method.	the wights are initialised either randomly or by using some statistical prpoperties ofthe data like xavier initialisation	0.0031809676	0.99257815	0.0042410837
71	there are four layers in cnn.convolutional layer - the layer that performs a convolutional operation, creating several smaller picture windows to go over the data.relu layer - it brings non-linearity to the network and converts all the negative pixels to zero. the output is a rectified feature map.pooling layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map.fully connected layer - this layer recognizes and classifies the objects in the image.	the different layers are convolutional layer, pooling layers, activation layer and fully connected layers.	0.16876885	0.5517562	0.2794749
72	pooling is used to reduce the spatial dimensions of a cnn. it performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix.	pooling is used to reduce the spatial dimension of the output while trying to maintain the activation in the regions. 	0.011615185	0.8952089	0.09317589
73	long-short-term memory (lstm) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. there are three steps in an lstm network: step 1: the network decides what to forget and what to remember. step 2: it selectively updates cell state values. step 3: the network decides what part of the current state makes it to the output.	lstm is a kind of rnn capable of learning long-term relationships, remembering information for long periods as its default behavior.	0.9743328	0.018225051	0.007442319
74	while training an rnn, your slope can become either too small or too large; this makes the training difficult. when the slope is too small, the problem is known as a “vanishing gradient.” when the slope tends to grow exponentially instead of decaying, it’s referred to as an “exploding gradient.” gradient problems lead to long training times, poor performance, and low accuracy.	vanishing and exploding gradients are problems encountered in deep neural networks during back propagation. in vanishing gradient descent the gradient becomes negligible in the initial layers thus weights are not updqated. in exploding gradient descent the weights keep on increasing or decreasing.	0.008439118	0.8408705	0.15069044
75	epoch - represents one iteration over the entire dataset (everything put into the training model).batch - refers to when we cannot pass the entire dataset into the neural network at once, so we divide the dataset into several batches.iteration - if we have 10,000 images as data and a batch size of 200. then an epoch should run 50 iterations (10,000 divided by 50).	an epoch is one run of the model with forward and back propagation. batch defines the amount of data to be used in one iteration. iteration is one run of the model of batch size amount of data.	0.6846483	0.2688629	0.04648875
76	tensorflow provides both c++ and python apis, making it easier to work on and has a faster compilation time compared to other deep learning libraries like keras and torch. tensorflow supports both cpu and gpu computing devices.	it's not. according to recent statistics, most papers in the field today use pytorch instead.	0.00044272328	0.36965632	0.62990093
77	a tensor is a mathematical object represented as arrays of higher dimensions. these arrays of data with different dimensions and ranks fed as input to the neural network are called “tensors.”	a tensor is a mathematical entity which represents vectors of higher dimension.	0.7455778	0.06807997	0.18634227
78	everything in a tensorflow is based on creating a computational graph. it has a network of nodes where each node operates, nodes represent mathematical operations, and edges represent tensors. since data flows in the form of a graph, it is also called a “dataflow graph.”	all the computations can be represented as data flows. and the data flows throw operations of variables, thus all the tensor operations can be visualised using graphs.	0.29941794	0.64936155	0.051220417
79	suppose there is a wine shop purchasing wine from dealers, which they resell later. but some dealers sell fake wine. in this case, the shop owner should be able to distinguish between fake and authentic wine.the forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. the shop owner would probably get some feedback from wine experts that some of the wine is not original. the owner would have to improve how he determines whether a wine is fake or authentic.the forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.let us understand this example with the help of an image shown above.there is a noise vector coming into the forger who is generating fake wine.here the forger acts as a generator.the shop owner acts as a discriminator.the discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. the shop owner has to figure out whether it is real or fake.so, there are two primary components of generative adversarial network (gan) named,generator and discriminatorthe generator is a cnn that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images the ultimate aim is to make the discriminator learn to identify real and fake images.	the generator is a cnn that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images the ultimate aim is to make the discriminator learn to identify real and fake images.	0.25251853	0.38027462	0.3672068
80	this neural network has three layers in which the input neurons are equal to the output neurons. the network's target outside is the same as the input. it uses dimensionality reduction to restructure the input. it works by compressing the image input to a latent space representation then reconstructing the output from this representation.	autoencoder is an automatic encoder. it gives the the embedding vectors without even training. it directly give the output encodings by learning the input. it only consists of an encoder.	0.0030495962	0.04416662	0.95278376
81	bagging and boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.with bagging, we take a dataset and split it into training data and test data. then we randomly select data to place into the bags and train the model separately.with boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy.	bagging is the technique of training a model, another model to predict its error, another to pedict error of errors and so on till we require and ensambling them. boosting is the method of predicting using different models and taking maximum voting or mean of the results.	0.8285478	0.055395696	0.116056584
82	the fourier transform function efficiently analyzes, maintains, and manages large datasets. you can use it to generate real-time array data that is helpful for processing multiple signals.	its a very good transform, and it makes things very easier.	0.017916175	0.98180276	0.00028101914
83	transfer learning is the process of transferring the learning from a model to another model without having to train it from scratch. it takes critical parts of a pre-trained model and applies them to solve new but similar machine learning problems.	transfer learning is the process of having a pre-trained model and then fine tuning it to a specific task by freezing most layers and training just some of the final layers.	0.0024117334	0.40611216	0.59147614
84	vgg-16,bert,gtp-3,inception v3 and xception.	roberta, bert	0.0044590915	0.022765357	0.97277546
85	autoencoders are used to convert black and white images into colored images.autoencoder helps to extract features and hidden patterns in the data.it is also used to reduce the dimensionality of data.it can also be used to remove noises from images.	extract features from data and even reduce dimensions of the data.	0.16730049	0.82980055	0.0028989871
86	mini-batch gradient is highly efficient compared to stochastic gradient descent.it lets you attain generalization by finding the flat minima.mini-batch gradient helps avoid local minima to allow gradient approximation for the whole dataset.	mini-batch gradient descent provides a trade-off between stochastic gradient descent and batvh gradient descent. batch gradient descent better results than stochastic gradient descent but requires high memory. thus mini batch gradient practically provides the best results.	0.011843116	0.9582575	0.029899452
87	leaky relu is an advanced version of the relu activation function. in general, the relu function defines the gradient to be 0 when all the values of inputs are less than zero. this deactivates the neurons. to overcome this problem, leaky relu activation functions are used. it has a very small slope for negative values instead of a flat slope.	leaky relu is relu with noise added to it thus the name leaky.	0.01056007	0.96391124	0.02552874
88	data augmentation is the process of creating new data by enhancing the size and quality of training datasets to ensure better models can be built using them. there are different techniques to augment data such as numerical data augmentation, image augmentation, gan-based augmentation, and text augmentation.	data augmentation is the method of increasing the data by using various methods like cropping, flipping, etc.	0.004563436	0.20196952	0.793467
89	adaptive moment estimation or adam optimization is an extension to the stochastic gradient descent. this algorithm is useful when working with complex problems involving vast amounts of data or parameters. it needs less memory and is efficient. 	its just an extension and modified version of stochastic gradient descent.	0.07844787	0.635596	0.28595608
90	the number of parameters in a convolutional neural network is much more diminutive than that of a dense neural network. hence, a cnn is less likely to overfit.cnn allows you to look at the weights of a filter and visualize what the network learned. so, this gives a better understanding of the model.cnn trains models in a hierarchical way, i.e., it learns the patterns by explaining complex patterns using simpler ones.	convolutional neural network is preferred over dense neural networks for image data because they take the spatial information into the consideration. also less calculations are required.	0.0031375687	0.9693171	0.027545357
91	use the relu activation function instead of the sigmoid function.initialize neural networks using xavier initialization that works with tanh activation.	we can deal with vanishing gradient problems by making the neural networks shallower.	0.0030594843	0.048659023	0.94828147
92	both deep and shallow neural networks can approximate the values of a function. but the deep neural network is more efficient as it learns something new in every layer. a shallow neural network has only one hidden layer. but a deep neural network has several hidden layers that create a deeper representation and computation capability.	deep neural networks consists of more layers than shallow neural networks. as a result the deep neural networks are capable of learning more complex mathematcal relations. thus deep networks are better provided there is enough computational resources.	0.0032643762	0.9936232	0.0031126163
93	if you set the weights to zero, then every neuron at each layer will produce the same result and the same gradient value during backpropagation. so, the neural network won’t be able to learn the function as there is no asymmetry between the neurons. hence, randomness to the weight initialization process is crucial.	we need to add randomness to the weights to ensure not all of them are zero simultaneiusly	0.5675453	0.37031913	0.06213554
94	hyperparameters in a neural network can be trained using four components:batch size: indicates the size of the input data.epochs: denotes the number of times the training data is visible to the neural network to train. momentum: used to get an idea of the next steps that occur with the data being executed.learning rate: represents the time required for the network to update the parameters and learn.	hyperparameters in a neural can be trained using backpropagation.	0.008184135	0.10592531	0.8858907
95	it's a deep learning procedure in which a model is fed raw data and the entire data is trained at the same time to create the desired result with no intermediate steps. it is a deep learning method in which all of the different steps are trained simultaneously rather than sequentially. end-to-end learning has the advantage of eliminating the requirement for implicit feature engineering, which usually results in lower bias. driverless automobiles are an excellent example that you may use in your end-to-end learning content. they are guided by human input and are programmed to learn and interpret information automatically using a cnn to fulfill tasks. another good example is the generation of a written transcript (output) from a recorded audio clip (input). the model here skips all of the steps in the middle, focusing instead on the fact that it can manage the entire sequence of steps and tasks.	end to end learning is learning a model which takes input at one end and gives output at the other.	0.21934362	0.4679341	0.31272238
96	gradient clipping is a technique for dealing with the problem of exploding gradients (a situation in which huge error gradients build up over time, resulting in massive modifications to neural network model weights during training) that happens during backpropagation. the problem of exploding gradients occurs when the gradients get excessively big during training, causing the model to become unstable. if the gradient has crossed the anticipated range, the gradient values are driven element-by-element to a specific minimum or maximum value. gradient clipping improves numerical stability while training a neural network, but it has little effect on the performance of the model.	gradient clipping is the process of cipping the gradient by setting a maximum value in order to avoid exploding gradients	0.0012960919	0.65700024	0.34170368
97	yes, even if all of the biases are set to zero, the neural network model has a chance of learning. no, training a model by setting all of the weights to 0 is impossible since the neural network will never learn to complete a task. when all weights are set to zero, the derivatives for each w remain constant, resulting in neurons learning the same features in each iteration. any constant initialization of weights, not simply zero, is likely to generate a poor result.	yes it is possible to train a model by setting all biases to zero. it is also possible to train them by setting all weights to 0.	0.016101092	0.3872862	0.5966128
