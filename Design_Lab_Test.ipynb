{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electric-engineer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 19 20:04:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    24W / 250W |      6MiB / 12198MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    36W / 250W |   6711MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1394      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      1394      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2374319      C   python                           2517MiB |\n",
      "|    1   N/A  N/A   2382047      C   ...hal/miniconda3/bin/python     2197MiB |\n",
      "|    1   N/A  N/A   2440248      C   python                           1991MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incoming-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, copy, time, sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "import tqdm\n",
    "\n",
    "from transformers import BertForPreTraining, BertModel, BertConfig, BertTokenizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incoming-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriental-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "processed-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "welsh-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert=False):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.PAD = tokenizer.pad_token_id\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)\n",
    "\n",
    "        \n",
    "    def forward(self, C, c_labels):\n",
    "\n",
    "        output = self.bert(\n",
    "            input_ids = C, \n",
    "            attention_mask = (C!=self.PAD),\n",
    "            labels = c_labels\n",
    "        )   \n",
    "        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controlling-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_classifier = BertClassifier()\n",
    "bert_classifier.load_state_dict(torch.load('bert-parameters.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dynamic-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pd.read_csv(\"QA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "champion-carry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNo</th>\n",
       "      <th>Question</th>\n",
       "      <th>Gold standard answer</th>\n",
       "      <th>Dummy answer</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>What is deep learning?</td>\n",
       "      <td>Deep learning is a part of machine learning wi...</td>\n",
       "      <td>Profound learning may be a portion of machine ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>What are the main differences between AI, Mach...</td>\n",
       "      <td>AI stands for Artificial Intelligence. It is a...</td>\n",
       "      <td>AI stands for Manufactured Insights. It may be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Differentiate supervised and unsupervised deep...</td>\n",
       "      <td>Supervised learning is a system in which both ...</td>\n",
       "      <td>Administered learning could be a framework in ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Do you think that deep network is better than ...</td>\n",
       "      <td>Both shallow and deep networks are good enough...</td>\n",
       "      <td>Both shallow and deep networks are capable of ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>What do you mean by \"overfitting\"?</td>\n",
       "      <td>Overfitting is the most common issue which occ...</td>\n",
       "      <td>It ordinarily happens when a profound learning...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SNo                                           Question  \\\n",
       "0    1.0                             What is deep learning?   \n",
       "1    2.0  What are the main differences between AI, Mach...   \n",
       "2    3.0  Differentiate supervised and unsupervised deep...   \n",
       "3    4.0  Do you think that deep network is better than ...   \n",
       "4    5.0                 What do you mean by \"overfitting\"?   \n",
       "..   ...                                                ...   \n",
       "107  NaN                                                NaN   \n",
       "108  NaN                                                NaN   \n",
       "109  NaN                                                NaN   \n",
       "110  NaN                                                NaN   \n",
       "111  NaN                                                NaN   \n",
       "\n",
       "                                  Gold standard answer  \\\n",
       "0    Deep learning is a part of machine learning wi...   \n",
       "1    AI stands for Artificial Intelligence. It is a...   \n",
       "2    Supervised learning is a system in which both ...   \n",
       "3    Both shallow and deep networks are good enough...   \n",
       "4    Overfitting is the most common issue which occ...   \n",
       "..                                                 ...   \n",
       "107                                                NaN   \n",
       "108                                                NaN   \n",
       "109                                                NaN   \n",
       "110                                                NaN   \n",
       "111                                                NaN   \n",
       "\n",
       "                                          Dummy answer Unnamed: 4  \n",
       "0    Profound learning may be a portion of machine ...        NaN  \n",
       "1    AI stands for Manufactured Insights. It may be...        NaN  \n",
       "2    Administered learning could be a framework in ...        NaN  \n",
       "3    Both shallow and deep networks are capable of ...        NaN  \n",
       "4    It ordinarily happens when a profound learning...        NaN  \n",
       "..                                                 ...        ...  \n",
       "107                                                NaN        NaN  \n",
       "108                                                NaN        NaN  \n",
       "109                                                NaN        NaN  \n",
       "110                                                NaN        NaN  \n",
       "111                                                NaN             \n",
       "\n",
       "[112 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handmade-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.drop([\"Unnamed: 4\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "round-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "operational-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.drop([\"SNo\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "arabic-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = qa.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "attractive-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.drop([\"index\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "naked-steering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Gold standard answer</th>\n",
       "      <th>Dummy answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is deep learning?</td>\n",
       "      <td>Deep learning is a part of machine learning wi...</td>\n",
       "      <td>Profound learning may be a portion of machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main differences between AI, Mach...</td>\n",
       "      <td>AI stands for Artificial Intelligence. It is a...</td>\n",
       "      <td>AI stands for Manufactured Insights. It may be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Differentiate supervised and unsupervised deep...</td>\n",
       "      <td>Supervised learning is a system in which both ...</td>\n",
       "      <td>Administered learning could be a framework in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you think that deep network is better than ...</td>\n",
       "      <td>Both shallow and deep networks are good enough...</td>\n",
       "      <td>Both shallow and deep networks are capable of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you mean by \"overfitting\"?</td>\n",
       "      <td>Overfitting is the most common issue which occ...</td>\n",
       "      <td>It ordinarily happens when a profound learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>What is the need to add randomness in the weig...</td>\n",
       "      <td>If you set the weights to zero, then every neu...</td>\n",
       "      <td>We need to add randomness to the weights to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>How can you train hyperparameters in a neural ...</td>\n",
       "      <td>Hyperparameters in a neural network can be tra...</td>\n",
       "      <td>Hyperparameters in a neural can be trained usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What do you mean by end-to-end learning?</td>\n",
       "      <td>It's a deep learning procedure in which a mode...</td>\n",
       "      <td>End to end learning is learning a model which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What do you understand about gradient clipping...</td>\n",
       "      <td>Gradient Clipping is a technique for dealing w...</td>\n",
       "      <td>Gradient Clipping is the process of cipping th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Is it possible to train a neural network model...</td>\n",
       "      <td>Yes, even if all of the biases are set to zero...</td>\n",
       "      <td>Yes it is possible to train a model by setting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                              What is deep learning?   \n",
       "1   What are the main differences between AI, Mach...   \n",
       "2   Differentiate supervised and unsupervised deep...   \n",
       "3   Do you think that deep network is better than ...   \n",
       "4                  What do you mean by \"overfitting\"?   \n",
       "..                                                ...   \n",
       "93  What is the need to add randomness in the weig...   \n",
       "94  How can you train hyperparameters in a neural ...   \n",
       "95           What do you mean by end-to-end learning?   \n",
       "96  What do you understand about gradient clipping...   \n",
       "97  Is it possible to train a neural network model...   \n",
       "\n",
       "                                 Gold standard answer  \\\n",
       "0   Deep learning is a part of machine learning wi...   \n",
       "1   AI stands for Artificial Intelligence. It is a...   \n",
       "2   Supervised learning is a system in which both ...   \n",
       "3   Both shallow and deep networks are good enough...   \n",
       "4   Overfitting is the most common issue which occ...   \n",
       "..                                                ...   \n",
       "93  If you set the weights to zero, then every neu...   \n",
       "94  Hyperparameters in a neural network can be tra...   \n",
       "95  It's a deep learning procedure in which a mode...   \n",
       "96  Gradient Clipping is a technique for dealing w...   \n",
       "97  Yes, even if all of the biases are set to zero...   \n",
       "\n",
       "                                         Dummy answer  \n",
       "0   Profound learning may be a portion of machine ...  \n",
       "1   AI stands for Manufactured Insights. It may be...  \n",
       "2   Administered learning could be a framework in ...  \n",
       "3   Both shallow and deep networks are capable of ...  \n",
       "4   It ordinarily happens when a profound learning...  \n",
       "..                                                ...  \n",
       "93  We need to add randomness to the weights to en...  \n",
       "94  Hyperparameters in a neural can be trained usi...  \n",
       "95  End to end learning is learning a model which ...  \n",
       "96  Gradient Clipping is the process of cipping th...  \n",
       "97  Yes it is possible to train a model by setting...  \n",
       "\n",
       "[98 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "frank-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class data():\n",
    "      \n",
    "  def __init__(self, max_len):\n",
    "    super(data, self).__init__()\n",
    "    self.data=[]\n",
    "\n",
    "    premise = qa[\"Gold standard answer\"].tolist()\n",
    "    hypo = qa[\"Dummy answer\"].tolist()\n",
    "    length = len(premise)\n",
    "\n",
    "    for i in tqdm.auto.tqdm(range(length)):\n",
    "        \n",
    "        premise_cleaned=premise[i].replace(\"\\n\",\"\")\n",
    "        premise_cleaned=re.sub(' +',' ',premise_cleaned)\n",
    "        premise_cleaned=premise_cleaned.lower()\n",
    "\n",
    "        hypo_cleaned=hypo[i].replace(\"\\n\",\"\")\n",
    "        hypo_cleaned=re.sub(' +',' ',hypo_cleaned)\n",
    "        hypo_cleaned=hypo_cleaned.lower()\n",
    "\n",
    "        sent = \"[CLS]\" + premise_cleaned + \"[SEP]\" + hypo_cleaned + \"[SEP]\"\n",
    "        enc_sent = tokenizer(sent, padding='max_length', add_special_tokens=False)['input_ids']\n",
    "        self.data.append(torch.tensor(enc_sent[:max_len]))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index]\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "patent-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf35bc715b6d4bf2a317c2c8e390407e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qa_data = data(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hollywood-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "waiting-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = []\n",
    "for i in range(len(qa_data)):\n",
    "  output = bert_classifier(torch.unsqueeze(qa_data[i],0),None)\n",
    "  outputs = output[0].detach().cpu().numpy()\n",
    "  prob = softmax(outputs)\n",
    "  prob_list.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sustained-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = qa[\"Gold standard answer\"].tolist()\n",
    "hypo = qa[\"Dummy answer\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "clinical-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qa_results.tsv', 'w') as f:\n",
    "    f.write(\"S.No.\"+\"\\t\"+ \"Gold Standard Answer\" + \"\\t\" +\"Student Answer\" + \"\\t\" +\"Prob for Entailment\" + \"\\t\" + \"Prob for Neutral\" + \"\\t\" + \"Prob for Contradiction\" + \"\\n\")\n",
    "    for i in range(len(premise)):\n",
    "        premise_cleaned=premise[i].replace(\"\\n\",\"\")\n",
    "        premise_cleaned=re.sub(' +',' ',premise_cleaned)\n",
    "        premise_cleaned=premise_cleaned.lower()\n",
    "\n",
    "        hypo_cleaned=hypo[i].replace(\"\\n\",\"\")\n",
    "        hypo_cleaned=re.sub(' +',' ',hypo_cleaned)\n",
    "        hypo_cleaned=hypo_cleaned.lower()\n",
    "\n",
    "        f.write(str(i) + \"\\t\" + premise_cleaned + \"\\t\" + hypo_cleaned + \"\\t\" + str(prob_list[i][0][0]) + \"\\t\" + str(prob_list[i][0][1]) + \"\\t\" + str(prob_list[i][0][2]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
